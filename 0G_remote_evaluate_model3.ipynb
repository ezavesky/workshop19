{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of simple model load and evaluate\n",
    "\n",
    "# ===============LICENSE_START=======================================================\n",
    "# Apache-2.0\n",
    "# ===================================================================================\n",
    "# Copyright (C) 2019 AT&T Intellectual Property  All rights reserved.\n",
    "# ===================================================================================\n",
    "# This software file is distributed by AT&T\n",
    "# under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# This file is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ===============LICENSE_END=========================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,sys,shutil  # file checks\n",
    "import dill as pickle   # serialize functions and data as compressed binary \n",
    "import gzip  # compression \n",
    "import yaml   # configuration file\n",
    "import time  # time tracking\n",
    "\n",
    "import threading  # threaded process evals\n",
    "\n",
    "from acumos.wrapped import load_model\n",
    "from acumos.modeling import Model, List, Dict, create_namedtuple, create_dataframe\n",
    "from acumos.session import AcumosSession, Requirements\n",
    "\n",
    "import util_call\n",
    "import util_review\n",
    "\n",
    "# load our configuration\n",
    "config_path = 'config.yaml'\n",
    "if not os.path.isfile(config_path):\n",
    "    print(\"Sorry, can't find the configuration file {}, aborting.\".format(config_path))\n",
    "    sys.exit(-1)\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data\n",
    "Load the raw test data and double-check the schema of the data with a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['helpful', 'reviewText', 'summary', 'unixReviewTime', 'categories',\n",
      "       'description'],\n",
      "      dtype='object')\n",
      "                                                            17315\n",
      "helpful                                                    [0, 0]\n",
      "reviewText      I am happy that this product is made from 50 %...\n",
      "summary                                                Green dock\n",
      "unixReviewTime                                         1326326400\n",
      "categories      [office products, office & school supplies, de...\n",
      "description     The Soho Silver Docking Station features three...\n"
     ]
    }
   ],
   "source": [
    "## PART 1 - load and start a local model runner \n",
    "# https://pypi.org/project/acumos/#using-dataframes-with-scikit-learn\n",
    "\n",
    "# read our larger datasets as binary files\n",
    "with gzip.open(config[\"path\"][\"etl\"], 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "print(df[\"X_test\"].columns)\n",
    "print(df[\"X_test\"].sample(1).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create wrapped model protoype\n",
    "Future versions of the API are addressing this issue, but for now, we'll need to mock-up what the call structure looks like for a given model.  For example, check the `Model Prototype Definition` section from the last  notebook for some additional discussion.  \n",
    "\n",
    "*NOTE*: The most natural way to do get a model's signature and connection data is to find it on the marketplace and download the required files (e.g. protobuf definition, etc) from there directly.\n",
    "\n",
    "Looking at a few example models for text-based sentiment processing we see a few common types there as well.\n",
    "\n",
    "* **text-to-float** pattern: a textual string is input for the output of class probabilites\n",
    "> TextIn = create_namedtuple('TextIn', [(\"TextIn\", str)])\n",
    "  FloatOut = create_namedtuple('FloatOut', [(\"FloatOut\", List[float])])\n",
    "\n",
    "* **text-to-float** pattern: a textual string is input for the output of class probabilites\n",
    "> TextIn = create_namedtuple('TextIn', [(\"TextIn\", str)])\n",
    "  FloatOut = create_namedtuple('FloatOut', [(\"FloatOut\", List[float])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through and create just a few model templates\n",
    "TextIn = create_namedtuple('TextIn', [(\"TextIn\", str)])\n",
    "FloatOut = create_namedtuple('FloatOut', [(\"FloatOut\", List[float])])\n",
    "\n",
    "# create function templates\n",
    "def sent_predict(df: TextIn) -> FloatOut:\n",
    "    '''Dummy function for prediction of a sentice'''\n",
    "    return FloatOut([])\n",
    "model = Model(sent_predict=sent_predict, classify=sent_predict)\n",
    "\n",
    "# create model so that we can run it locally\n",
    "session = AcumosSession()\n",
    "model_dump = config[\"publish\"][\"name_model3\"]+\"_\"+\"text-to-float\"\n",
    "path_dump = os.path.join('data', model_dump)\n",
    "if os.path.exists(path_dump):\n",
    "    shutil.rmtree(path_dump)\n",
    "session.dump(model, model_dump, 'data')  # creates ~/<name_publish>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Evaluate a Sentiment Model\n",
    "Now that we have the model prototype \n",
    "\n",
    "1. Iterate through which text models (the shared ones) we want to analyze\n",
    "\n",
    "2. For the raw training data and test data (the places where we have raw textual reviews), convienently wrapped in the helper function `call_sentiment_helper`\n",
    "    1. Load the right stubbed model template (the one we just saved to disk above)\n",
    "    2. Sub-sample the raw input data if a max number of items was provided (this speeds up the local demo)\n",
    "    3. Call our model at a remote URL with the input data\n",
    "    4. Depending on the model template (the call pattern), pull out specific floating values to keep (flatten)\n",
    "    5. Return results\n",
    "\n",
    "3. With the above results, write them to disk if it was a full dataset (because it takes a while) or display them to verify that we're doing the right thing!\n",
    "\n",
    "4. Finally, cooalte the different results from each sentiment processor into a final data dictionary that other notebook scripts can utilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Started processing for model 'yelp'... === \n",
      "=== Started processing for model 'care'... === \n",
      "Started processing data... (10 of 5075 samples)Started processing data... (10 of 5075 samples)\n",
      "\n",
      "=== Started processing for model 'twitter'... === \n",
      "Started processing data... (10 of 5075 samples)\n"
     ]
    }
   ],
   "source": [
    "# first, we define a helper function that will load a model and call it against data\n",
    "def call_sentiment_helper(model_name, df_eval, col_process, max_process_items, config, wrapped_model=None):\n",
    "    # load model from disk, see that it is a nicely \"wrapped\" model\n",
    "    model_remote_param = config[\"sentiment\"][model_name]\n",
    "    model_dump = config[\"publish\"][\"name_model3\"]+\"_\"+model_remote_param[\"style\"]\n",
    "    \n",
    "    # we allow the model to be passed because (a) they're all the same, (b) threading breaks with sessions\n",
    "    if wrapped_model is None:\n",
    "        wrapped_model = load_model(os.path.join('data', model_dump))\n",
    "\n",
    "    # although there are a few text columns, we'll just send the the column `reviewText` in for analysis\n",
    "    # NOTE: we're \"wrapping\" the one column as well for standard calling structure\n",
    "    #    nd_sample = [ [text1], [text2], ... ]\n",
    "    idx_access = list(range(len(df_eval)))\n",
    "    if max_process_items != 0:   # 0 special case for EVERYTHING\n",
    "        np.random.shuffle(idx_access)\n",
    "        idx_access = idx_access[:min(len(idx_access), max_process_items)]\n",
    "    print(\"Started processing data... ({} of {} samples)\".format(len(idx_access), len(df_eval)))\n",
    "    nd_sample = [[wrap_item] for wrap_item in df_eval.iloc[idx_access][col_process].values.tolist()]\n",
    "    list_result, list_idx = util_call.score_model(wrapped_model, nd_sample, False,\n",
    "                        name_function=model_remote_param[\"api\"],\n",
    "                        url_remote=\"{}:{}\".format(\n",
    "                            config[\"sentiment\"][\"deploy_host\"], model_remote_param[\"port\"]))\n",
    "    index_df = [idx_access[i] for i in list_idx]  # remap our index in case anything was missed!\n",
    "    df_result = pd.DataFrame(list_result, columns=[col_process], index=df_eval.index[index_df])\n",
    "    \n",
    "    # now pull out the iteresting parts according to known style/output\n",
    "    if model_remote_param[\"style\"] == \"text-to-float\":\n",
    "        df_result = pd.DataFrame(df_result[\"FloatOut\"], columns=[col_process], index=df_result.index)\n",
    "    # other styles....\n",
    "    return df_result\n",
    "\n",
    "def helper_thread(model_name, wrapped_model=None):\n",
    "    print(\"=== Started processing for model '{}'... === \".format(model_name))\n",
    "    path_train = config[\"sentiment\"][model_name][\"path\"].format(\"train\")\n",
    "    path_test = config[\"sentiment\"][model_name][\"path\"].format(\"test\")\n",
    "    if not os.path.exists(path_test) or not os.path.exists(path_train) or max_process_items!=0:\n",
    "        df_test = call_sentiment_helper(model_name, df[\"X_test\"], \n",
    "            sentiment[\"col_sentiment\"], max_process_items, config, wrapped_model=wrapped_model)\n",
    "\n",
    "        df_train = call_sentiment_helper(model_name, df[\"X_train_raw\"], \n",
    "            sentiment[\"col_sentiment\"], max_process_items, config, wrapped_model=wrapped_model)\n",
    "        \n",
    "        # write out our larger datasets as binary files\n",
    "        if max_process_items==0:    # only write full datasets\n",
    "            df_test.to_csv(path_train, index=True, headers=True)\n",
    "            df_train.to_csv(path_train, index=True, headers=True)\n",
    "        # show a preview of what was just done...\n",
    "        print(\"... sample for model '{}'\".format(model_name))\n",
    "        print(df_local[\"X_train_raw\"].join(df_train).sample(3))\n",
    "\n",
    "# okay, let's get ready to call our helper function for requested models\n",
    "sentiment = {}\n",
    "sentiment[\"X_test\"] = pd.DataFrame([], index=df[\"X_test\"].index)\n",
    "sentiment[\"X_train\"] = pd.DataFrame([], index=df[\"X_train_raw\"].index)\n",
    "sentiment[\"col_sentiment\"] = \"reviewText\"\n",
    "\n",
    "# truncate range for faster evaluation\n",
    "max_process_items = 10     # set to 0 for everything (warning it might take a while)\n",
    "\n",
    "# actual evaluation code...\n",
    "thread_list = []\n",
    "thread_utilize = True\n",
    "# load model (WARNING: if you needed another model style than what was created, you may need to rework this)\n",
    "wrapped_model = load_model(os.path.join('data', model_dump))\n",
    "# evaluate models that are activated/available\n",
    "for model_name in config[\"sentiment\"][\"active_model\"]:\n",
    "    if thread_utilize:       # creating thread\n",
    "        t1 = threading.Thread(target=helper_thread, args=(model_name,wrapped_model)) \n",
    "        t1.start()\n",
    "        thread_list.append(t1)\n",
    "    else:\n",
    "        helper_thread(model_name,wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time for 10 items, 0.696 sec\n",
      "Started processing data... (10 of 20299 samples)\n",
      "Evaluation time for 10 items, 0.757 sec\n",
      "Started processing data... (10 of 20299 samples)\n",
      "Evaluation time for 10 items, 0.921 sec\n",
      "Started processing data... (10 of 20299 samples)\n",
      "Evaluation time for 10 items, 0.676 sec\n",
      "... sample for model 'twitter'\n",
      "                      FloatOut  \\\n",
      "15932  [4.353042628073068e-06]   \n",
      "20758  [0.0012356454906100219]   \n",
      "3734       [0.006035399578248]   \n",
      "\n",
      "                                              reviewText  \n",
      "15932  The nice thing about this package opener is th...  \n",
      "20758  I love the rainbow of colors that this set has...  \n",
      "3734   I run a small business out of my home so I'm o...  \n",
      "Evaluation time for 10 items, 0.727 sec\n",
      "... sample for model 'yelp'\n",
      "                      FloatOut  \\\n",
      "21371    [0.00779726812467857]   \n",
      "19548  [8.589346390654782e-34]   \n",
      "9533    [0.007811815350175308]   \n",
      "\n",
      "                                              reviewText  \n",
      "21371  The price for the printer is great, especially...  \n",
      "19548  This shredder is very expensive but as the old...  \n",
      "9533   Does your four to seven year-old child need so...  \n",
      "Evaluation time for 10 items, 0.914 sec\n",
      "... sample for model 'care'\n",
      "                    FloatOut  \\\n",
      "19459   [0.3305971026420593]   \n",
      "464    [0.19624054431915283]   \n",
      "9151    [0.5583804845809937]   \n",
      "\n",
      "                                              reviewText  \n",
      "19459  I've tried this out making over-sized playing ...  \n",
      "464    I have read innumerable hardback novels since ...  \n",
      "9151   This pocket filer worked great until it didn't...  \n"
     ]
    }
   ],
   "source": [
    "# wait for all threads to terminate\n",
    "for i in range(len(thread_list)):\n",
    "    thread_list[i].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining and writing combined features to output ETL file...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.core' has no attribute '_multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4da5b962fe4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mdf_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_local\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_local\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/cognita36/lib/python3.6/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, ignore)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;31m# apply kwd settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0mpik\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpik\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_main_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/cognita36/lib/python3.6/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36m_import_module\u001b[0;34m(import_name, safe)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.core' has no attribute '_multiarray_umath'"
     ]
    }
   ],
   "source": [
    "print(\"Combining and writing combined features to output ETL file...\")\n",
    "# now read the processed samples into our main dataframe\n",
    "for model_name in config[\"sentiment\"][\"active_model\"]:\n",
    "    path_train = config[\"sentiment\"][model_name][\"path\"].format(\"train\")\n",
    "    path_test = config[\"sentiment\"][model_name][\"path\"].format(\"test\")\n",
    "    if os.path.exists(path_train) and os.path.exists(path_test):\n",
    "        df_read = pd.read_csv(path_train)\n",
    "        sentiment[\"X_train\"] = sentiment[\"X_train\"].join(df_read)\n",
    "        df_read = pd.read_csv(path_train)\n",
    "        sentiment[\"X_test\"] = sentiment[\"X_test\"].join(df_read)\n",
    "                \n",
    "with gzip.open(config[\"path\"][\"sentiment\"], 'wb') as f:\n",
    "    pickle.dump(sentiment, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
