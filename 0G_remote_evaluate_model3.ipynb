{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of simple model load and evaluate\n",
    "\n",
    "# ===============LICENSE_START=======================================================\n",
    "# Apache-2.0\n",
    "# ===================================================================================\n",
    "# Copyright (C) 2019 AT&T Intellectual Property  All rights reserved.\n",
    "# ===================================================================================\n",
    "# This software file is distributed by AT&T\n",
    "# under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# This file is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ===============LICENSE_END=========================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,sys,shutil  # file checks\n",
    "import dill as pickle   # serialize functions and data as compressed binary \n",
    "import gzip  # compression \n",
    "import yaml   # configuration file\n",
    "import time  # time tracking\n",
    "\n",
    "import threading  # threaded process evals\n",
    "\n",
    "from acumos.wrapped import load_model\n",
    "from acumos.modeling import Model, List, Dict, create_namedtuple, create_dataframe\n",
    "from acumos.session import AcumosSession, Requirements\n",
    "\n",
    "import util_call\n",
    "import util_review\n",
    "\n",
    "# load our configuration\n",
    "config_path = 'config.yaml'\n",
    "if not os.path.isfile(config_path):\n",
    "    print(\"Sorry, can't find the configuration file {}, aborting.\".format(config_path))\n",
    "    sys.exit(-1)\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data\n",
    "Load the raw test data and double-check the schema of the data with a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['helpful', 'reviewText', 'summary', 'unixReviewTime', 'categories',\n",
      "       'description'],\n",
      "      dtype='object')\n",
      "                                                            21380\n",
      "helpful                                                    [0, 1]\n",
      "reviewText      Epson has a solid reputation for quality print...\n",
      "summary                     Compact size, beautiful print quality\n",
      "unixReviewTime                                         1373068800\n",
      "categories      [office products, office electronics, printers...\n",
      "description                                                      \n"
     ]
    }
   ],
   "source": [
    "## PART 1 - load and start a local model runner \n",
    "# https://pypi.org/project/acumos/#using-dataframes-with-scikit-learn\n",
    "\n",
    "# read our larger datasets as binary files\n",
    "with gzip.open(config[\"path\"][\"etl\"], 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "print(df[\"X_test\"].columns)\n",
    "print(df[\"X_test\"].sample(1).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create wrapped model protoype\n",
    "Future versions of the API are addressing this issue, but for now, we'll need to mock-up what the call structure looks like for a given model.  For example, check the `Model Prototype Definition` section from the last  notebook for some additional discussion.  \n",
    "\n",
    "*NOTE*: The most natural way to do get a model's signature and connection data is to find it on the marketplace and download the required files (e.g. protobuf definition, etc) from there directly.\n",
    "\n",
    "Looking at a few example models for text-based sentiment processing we see a few common types there as well.\n",
    "\n",
    "* **text-to-float** pattern: a textual string is input for the output of class probabilites\n",
    "> TextIn = create_namedtuple('TextIn', [(\"TextIn\", str)])\n",
    "  FloatOut = create_namedtuple('FloatOut', [(\"FloatOut\", List[float])])\n",
    "\n",
    "* **text-to-float** pattern: a textual string is input for the output of class probabilites\n",
    "> TextIn = create_namedtuple('TextIn', [(\"TextIn\", str)])\n",
    "  FloatOut = create_namedtuple('FloatOut', [(\"FloatOut\", List[float])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through and create just a few model templates\n",
    "TextIn = create_namedtuple('TextIn', [(\"TextIn\", str)])\n",
    "FloatOut = create_namedtuple('FloatOut', [(\"FloatOut\", List[float])])\n",
    "\n",
    "# create function templates\n",
    "def sent_predict(df: TextIn) -> FloatOut:\n",
    "    '''Dummy function for prediction of a sentice'''\n",
    "    return FloatOut([])\n",
    "model = Model(sent_predict=sent_predict, classify=sent_predict)\n",
    "\n",
    "# create model so that we can run it locally\n",
    "session = AcumosSession()\n",
    "model_dump = config[\"publish\"][\"name_model3\"]+\"_\"+\"text-to-float\"\n",
    "path_dump = os.path.join('data', model_dump)\n",
    "if os.path.exists(path_dump):\n",
    "    shutil.rmtree(path_dump)\n",
    "session.dump(model, model_dump, 'data')  # creates ~/<name_publish>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Evaluate a Sentiment Model\n",
    "Now that we have the model prototype \n",
    "\n",
    "1. Iterate through which text models (the shared ones) we want to analyze\n",
    "\n",
    "2. For the raw training data and test data (the places where we have raw textual reviews), convienently wrapped in the helper function `call_sentiment_helper`\n",
    "    1. Load the right stubbed model template (the one we just saved to disk above)\n",
    "    2. Sub-sample the raw input data if a max number of items was provided (this speeds up the local demo)\n",
    "    3. Call our model at a remote URL with the input data\n",
    "    4. Depending on the model template (the call pattern), pull out specific floating values to keep (flatten)\n",
    "    5. Return results\n",
    "\n",
    "3. With the above results, write them to disk if it was a full dataset (because it takes a while) or display them to verify that we're doing the right thing!\n",
    "\n",
    "4. Finally, cooalte the different results from each sentiment processor into a final data dictionary that other notebook scripts can utilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Started processing for model 'yelp'... === === Started processing for model 'care'... === \n",
      "Started processing data... (5075 of 5075 samples)\n",
      "\n",
      "=== Started processing for model 'twitter'... === \n",
      "Sample 250...\n",
      "Output error (http://acumos-gpu.research.att.com:8763), exception (500 Server Error: INTERNAL SERVER ERROR for url: http://acumos-gpu.research.att.com:8763/classify)\n",
      "Sample 500...\n",
      "Sample 750...\n",
      "Sample 1000...\n",
      "Sample 1250...\n",
      "Sample 1500...\n",
      "Sample 1750...\n",
      "Sample 2000...\n",
      "Sample 2250...\n",
      "Sample 2500...\n",
      "Sample 2750...\n",
      "Sample 3000...\n",
      "Sample 3250...\n",
      "Sample 3500...\n",
      "Sample 3750...\n",
      "Sample 4000...\n",
      "Sample 4250...\n",
      "Sample 4500...\n",
      "Sample 4750...\n",
      "Sample 5000...\n",
      "Evaluation time for 5075 items, 904.437 sec\n",
      "Started processing data... (20299 of 20299 samples)\n",
      "Sample 250...\n",
      "Sample 500...\n",
      "Output error (http://acumos-gpu.research.att.com:8763), exception (500 Server Error: INTERNAL SERVER ERROR for url: http://acumos-gpu.research.att.com:8763/classify)\n",
      "Sample 750...\n",
      "Sample 1000...\n",
      "Sample 1250...\n",
      "Sample 1500...\n",
      "Sample 1750...\n",
      "Sample 2000...\n",
      "Sample 2250...\n",
      "Sample 2500...\n",
      "Output error (http://acumos-gpu.research.att.com:8763), exception (500 Server Error: INTERNAL SERVER ERROR for url: http://acumos-gpu.research.att.com:8763/classify)\n",
      "Sample 2750...\n",
      "Sample 3000...\n",
      "Sample 3250...\n",
      "Sample 3500...\n",
      "Sample 3750...\n",
      "Sample 4000...\n",
      "Sample 4250...\n",
      "Sample 4500...\n",
      "Sample 4750...\n",
      "Sample 5000...\n",
      "Sample 5250...\n",
      "Output error (http://acumos-gpu.research.att.com:8763), exception (500 Server Error: INTERNAL SERVER ERROR for url: http://acumos-gpu.research.att.com:8763/classify)\n",
      "Sample 5500...\n",
      "Output error (http://acumos-gpu.research.att.com:8763), exception (500 Server Error: INTERNAL SERVER ERROR for url: http://acumos-gpu.research.att.com:8763/classify)\n",
      "Sample 5750...\n",
      "Sample 6000...\n",
      "Sample 6250...\n",
      "Sample 6500...\n",
      "Sample 6750...\n",
      "Sample 7000...\n",
      "Sample 7250...\n",
      "Sample 7500...\n",
      "Sample 7750...\n",
      "Sample 8000...\n",
      "Sample 8250...\n",
      "Sample 8500...\n",
      "Sample 8750...\n",
      "Sample 9000...\n",
      "Sample 9250...\n",
      "Sample 9500...\n",
      "Sample 9750...\n",
      "Output error (http://acumos-gpu.research.att.com:8763), exception (500 Server Error: INTERNAL SERVER ERROR for url: http://acumos-gpu.research.att.com:8763/classify)\n",
      "Sample 10000...\n",
      "Sample 10250...\n",
      "Sample 10500...\n",
      "Sample 10750...\n",
      "Sample 11000...\n",
      "Sample 11250...\n",
      "Sample 11500...\n",
      "Sample 11750...\n",
      "Sample 12000...\n",
      "Sample 12250...\n",
      "Sample 12500...\n",
      "Sample 12750...\n",
      "Sample 13000...\n",
      "Sample 13250...\n",
      "Sample 13500...\n",
      "Sample 13750...\n",
      "Sample 14000...\n",
      "Sample 14250...\n",
      "Sample 14500...\n",
      "Sample 14750...\n",
      "Sample 15000...\n",
      "Sample 15250...\n",
      "Sample 15500...\n",
      "Sample 15750...\n",
      "Sample 16000...\n",
      "Sample 16250...\n",
      "Sample 16500...\n",
      "Sample 16750...\n",
      "Sample 17000...\n",
      "Sample 17250...\n",
      "Sample 17500...\n",
      "Output error (http://acumos-gpu.research.att.com:8763), exception (500 Server Error: INTERNAL SERVER ERROR for url: http://acumos-gpu.research.att.com:8763/classify)\n",
      "Sample 17750...\n",
      "Sample 18000...\n",
      "Sample 18250...\n",
      "Sample 18500...\n",
      "Sample 18750...\n",
      "Sample 19000...\n",
      "Sample 19250...\n",
      "Sample 19500...\n",
      "Sample 19750...\n",
      "Sample 20000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/urllib3/util/connection.py\", line 79, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/urllib3/util/connection.py\", line 69, in create_connection\n",
      "    sock.connect(sa)\n",
      "TimeoutError: [Errno 60] Operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 354, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/http/client.py\", line 1239, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/http/client.py\", line 1285, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/http/client.py\", line 1234, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/http/client.py\", line 1026, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/http/client.py\", line 964, in send\n",
      "    self.connect()\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/urllib3/connection.py\", line 196, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/urllib3/connection.py\", line 180, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e)\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x1a18c2fba8>: Failed to establish a new connection: [Errno 60] Operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/requests/adapters.py\", line 445, in send\n",
      "    timeout=timeout\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/urllib3/util/retry.py\", line 398, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='acumos-gpu.research.att.com', port=8763): Max retries exceeded with url: /classify (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a18c2fba8>: Failed to establish a new connection: [Errno 60] Operation timed out',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-13-f770a5310f6a>\", line 41, in helper_thread\n",
      "    sentiment[\"col_sentiment\"], max_process_items, config, wrapped_model=wrapped_model)\n",
      "  File \"<ipython-input-13-f770a5310f6a>\", line 23, in call_sentiment_helper\n",
      "    config[\"sentiment\"][\"deploy_host\"], model_remote_param[\"port\"]))\n",
      "  File \"/Users/quinone/Documents/projects/miracle/ml_hack2019/reviews/util_call.py\", line 92, in score_model\n",
      "    resp_data = requests.post(\"{}/{}\".format(url_remote, name_function), data=trans_in_pb_bytes)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/requests/api.py\", line 112, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/requests/api.py\", line 58, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/requests/sessions.py\", line 512, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/requests/sessions.py\", line 622, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/site-packages/requests/adapters.py\", line 513, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='acumos-gpu.research.att.com', port=8763): Max retries exceeded with url: /classify (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a18c2fba8>: Failed to establish a new connection: [Errno 60] Operation timed out',))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first, we define a helper function that will load a model and call it against data\n",
    "def call_sentiment_helper(model_name, df_eval, col_process, max_process_items, config, wrapped_model=None):\n",
    "    # load model from disk, see that it is a nicely \"wrapped\" model\n",
    "    model_remote_param = config[\"sentiment\"][model_name]\n",
    "    model_dump = config[\"publish\"][\"name_model3\"]+\"_\"+model_remote_param[\"style\"]\n",
    "    \n",
    "    # we allow the model to be passed because (a) they're all the same, (b) threading breaks with sessions\n",
    "    if wrapped_model is None:\n",
    "        wrapped_model = load_model(os.path.join('data', model_dump))\n",
    "\n",
    "    # although there are a few text columns, we'll just send the the column `reviewText` in for analysis\n",
    "    # NOTE: we're \"wrapping\" the one column as well for standard calling structure\n",
    "    #    nd_sample = [ [text1], [text2], ... ]\n",
    "    idx_access = list(range(len(df_eval)))\n",
    "    if max_process_items != 0:   # 0 special case for EVERYTHING\n",
    "        np.random.shuffle(idx_access)\n",
    "        idx_access = idx_access[:min(len(idx_access), max_process_items)]\n",
    "    print(\"Started processing data... ({} of {} samples)\".format(len(idx_access), len(df_eval)))\n",
    "    nd_sample = [[wrap_item] for wrap_item in df_eval.iloc[idx_access][col_process].values.tolist()]\n",
    "    list_result, list_idx = util_call.score_model(wrapped_model, nd_sample, False,\n",
    "                        name_function=model_remote_param[\"api\"],\n",
    "                        url_remote=\"{}:{}\".format(\n",
    "                            config[\"sentiment\"][\"deploy_host\"], model_remote_param[\"port\"]))\n",
    "    index_df = [idx_access[i] for i in list_idx]  # remap our index in case anything was missed!\n",
    "    df_result = pd.DataFrame(list_result, index=df_eval.index[index_df])\n",
    "    \n",
    "    # now pull out the iteresting parts according to known style/output\n",
    "    if model_remote_param[\"style\"] == \"text-to-float\":\n",
    "        df_result = pd.DataFrame(df_result[\"FloatOut\"], index=df_result.index)\n",
    "    # other styles....\n",
    "    return df_result\n",
    "\n",
    "def helper_thread(model_name, wrapped_model=None):\n",
    "    df_local = {}\n",
    "    print(\"=== Started processing for model '{}'... === \".format(model_name))\n",
    "    if not os.path.exists(config[\"sentiment\"][model_name][\"path\"]) or max_process_items!=0:\n",
    "        df_local[\"X_test\"] = call_sentiment_helper(model_name, df[\"X_test\"], \n",
    "            sentiment[\"col_sentiment\"], max_process_items, config, wrapped_model=wrapped_model)\n",
    "\n",
    "        df_local[\"X_train_raw\"] = call_sentiment_helper(model_name, df[\"X_train_raw\"], \n",
    "            sentiment[\"col_sentiment\"], max_process_items, config, wrapped_model=wrapped_model)\n",
    "        \n",
    "        # write out our larger datasets as binary files\n",
    "        if max_process_items==0:    # only write full datasets\n",
    "            with gzip.open(config[\"sentiment\"][model_name][\"path\"], 'wb') as f:\n",
    "                pickle.dump(df_local, f)\n",
    "        # show a preview of what was just done...\n",
    "        print(\"... sample for model '{}'\".format(model_name))\n",
    "        print(df_local[\"X_train_raw\"].join(df[\"X_train_raw\"][sentiment[\"col_sentiment\"]]).sample(3))\n",
    "\n",
    "# okay, let's get ready to call our helper function for requested models\n",
    "sentiment = {}\n",
    "sentiment[\"X_test\"] = pd.DataFrame([], index=df[\"X_test\"].index)\n",
    "sentiment[\"X_train_raw\"] = pd.DataFrame([], index=df[\"X_train_raw\"].index)\n",
    "sentiment[\"col_sentiment\"] = \"reviewText\"\n",
    "\n",
    "# truncate range for faster evaluation\n",
    "max_process_items = 0     # set to 0 for everything (warning it might take a while)\n",
    "\n",
    "# actual evaluation code...\n",
    "thread_list = []\n",
    "thread_utilize = True\n",
    "# load model (WARNING: if you needed another model style than what was created, you may need to rework this)\n",
    "wrapped_model = load_model(os.path.join('data', model_dump))\n",
    "# evaluate models that are activated/available\n",
    "for model_name in config[\"sentiment\"][\"active_model\"]:\n",
    "    if thread_utilize:       # creating thread\n",
    "        t1 = threading.Thread(target=helper_thread, args=(model_name,wrapped_model)) \n",
    "        t1.start()\n",
    "        thread_list.append(t1)\n",
    "    else:\n",
    "        helper_thread(model_name,wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 250...\n",
      "Output error (http://acumos-gpu.research.att.com:8763), exception (500 Server Error: INTERNAL SERVER ERROR for url: http://acumos-gpu.research.att.com:8763/classify)\n",
      "Sample 500...\n",
      "Sample 750...\n",
      "Sample 1000...\n",
      "Sample 1250...\n",
      "Sample 1500...\n",
      "Sample 1750...\n",
      "Sample 2000...\n",
      "Sample 2250...\n",
      "Sample 2500...\n",
      "Sample 2750...\n",
      "Sample 3000...\n",
      "Sample 3250...\n",
      "Sample 3500...\n",
      "Sample 3750...\n",
      "Sample 4000...\n",
      "Sample 4250...\n",
      "Sample 4500...\n",
      "Sample 4750...\n",
      "Sample 5000...\n",
      "Evaluation time for 5075 items, 901.101 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/quinone/anaconda/envs/cognita36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-1bd5cb67c98d>\", line 37, in helper_thread\n",
      "    sentiment[\"col_sentiment\"], max_process_items, config, wrapped_model=wrapped_model)\n",
      "  File \"<ipython-input-5-1bd5cb67c98d>\", line 24, in call_sentiment_helper\n",
      "    df_result = pd.DataFrame(list_result, index=df_eval.index[idx_access[list_idx]])\n",
      "TypeError: list indices must be integers or slices, not list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wait for all threads to terminate\n",
    "for i in range(len(thread_list)):\n",
    "    thread_list[i].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining and writing combined features to output ETL file...\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining and writing combined features to output ETL file...\")\n",
    "# now read the processed samples into our main dataframe\n",
    "for model_name in config[\"sentiment\"][\"active_model\"]:\n",
    "    if os.path.exists(config[\"sentiment\"][model_name][\"path\"]):\n",
    "        with gzip.open(config[\"sentiment\"][model_name][\"path\"], 'rb') as f:\n",
    "            df_local = pickle.load(f)\n",
    "            for k in df_local:\n",
    "                sentiment[k][model_name] = df_local[k]\n",
    "with gzip.open(config[\"path\"][\"sentiment\"], 'wb') as f:\n",
    "    pickle.dump(sentiment, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
