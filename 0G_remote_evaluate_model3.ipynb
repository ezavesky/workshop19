{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of simple model load and evaluate\n",
    "\n",
    "# ===============LICENSE_START=======================================================\n",
    "# Apache-2.0\n",
    "# ===================================================================================\n",
    "# Copyright (C) 2019 AT&T Intellectual Property  All rights reserved.\n",
    "# ===================================================================================\n",
    "# This software file is distributed by AT&T\n",
    "# under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# This file is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ===============LICENSE_END=========================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,sys,shutil  # file checks\n",
    "import dill as pickle   # serialize functions and data as compressed binary \n",
    "import gzip  # compression \n",
    "import yaml   # configuration file\n",
    "import time  # time tracking\n",
    "\n",
    "import threading  # threaded process evals\n",
    "\n",
    "from acumos.wrapped import load_model\n",
    "from acumos.modeling import Model, List, Dict, create_namedtuple, create_dataframe\n",
    "from acumos.session import AcumosSession, Requirements\n",
    "\n",
    "import util_call\n",
    "import util_review\n",
    "\n",
    "# load our configuration\n",
    "config_path = 'config.yaml'\n",
    "if not os.path.isfile(config_path):\n",
    "    print(\"Sorry, can't find the configuration file {}, aborting.\".format(config_path))\n",
    "    sys.exit(-1)\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data\n",
    "Load the raw test data and double-check the schema of the data with a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['helpful', 'reviewText', 'summary', 'unixReviewTime', 'categories',\n",
      "       'description'],\n",
      "      dtype='object')\n",
      "                                                             4318\n",
      "helpful                                                    [0, 0]\n",
      "reviewText      There is no way to ensure that your mail wont ...\n",
      "summary               Know when your mail has been tampered with!\n",
      "unixReviewTime                                         1277337600\n",
      "categories      [office products, office & school supplies, en...\n",
      "description     Reveal-n-Seal Envelopes with security tint fea...\n"
     ]
    }
   ],
   "source": [
    "## PART 1 - load and start a local model runner \n",
    "# https://pypi.org/project/acumos/#using-dataframes-with-scikit-learn\n",
    "\n",
    "# read our larger datasets as binary files\n",
    "with gzip.open(config[\"path\"][\"etl\"], 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "print(df[\"X_test\"].columns)\n",
    "print(df[\"X_test\"].sample(1).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create wrapped model protoype\n",
    "Future versions of the API are addressing this issue, but for now, we'll need to mock-up what the call structure looks like for a given model.  For example, check the `Model Prototype Definition` section from the last  notebook for some additional discussion.  \n",
    "\n",
    "*NOTE*: The most natural way to do get a model's signature and connection data is to find it on the marketplace and download the required files (e.g. protobuf definition, etc) from there directly.\n",
    "\n",
    "Looking at a few example models for text-based sentiment processing we see a few common types there as well.\n",
    "\n",
    "* **text-to-float** pattern: a textual string is input for the output of class probabilites\n",
    "> TextIn = create_namedtuple('TextIn', [(\"TextIn\", str)])\n",
    "  FloatOut = create_namedtuple('FloatOut', [(\"FloatOut\", List[float])])\n",
    "\n",
    "* **text-to-float** pattern: a textual string is input for the output of class probabilites\n",
    "> TextIn = create_namedtuple('TextIn', [(\"TextIn\", str)])\n",
    "  FloatOut = create_namedtuple('FloatOut', [(\"FloatOut\", List[float])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through and create just a few model templates\n",
    "TextIn = create_namedtuple('TextIn', [(\"TextIn\", str)])\n",
    "FloatOut = create_namedtuple('FloatOut', [(\"FloatOut\", List[float])])\n",
    "\n",
    "# create function templates\n",
    "def sent_predict(df: TextIn) -> FloatOut:\n",
    "    '''Dummy function for prediction of a sentice'''\n",
    "    return FloatOut([])\n",
    "model = Model(sent_predict=sent_predict, classify=sent_predict)\n",
    "\n",
    "# create model so that we can run it locally\n",
    "session = AcumosSession()\n",
    "model_dump = config[\"publish\"][\"name_model3\"]+\"_\"+\"text-to-float\"\n",
    "path_dump = os.path.join('data', model_dump)\n",
    "if os.path.exists(path_dump):\n",
    "    shutil.rmtree(path_dump)\n",
    "session.dump(model, model_dump, 'data')  # creates ~/<name_publish>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Evaluate a Sentiment Model\n",
    "Now that we have the model prototype \n",
    "\n",
    "1. Iterate through which text models (the shared ones) we want to analyze\n",
    "\n",
    "2. For the raw training data and test data (the places where we have raw textual reviews), convienently wrapped in the helper function `call_sentiment_helper`\n",
    "    1. Load the right stubbed model template (the one we just saved to disk above)\n",
    "    2. Sub-sample the raw input data if a max number of items was provided (this speeds up the local demo)\n",
    "    3. Call our model at a remote URL with the input data\n",
    "    4. Depending on the model template (the call pattern), pull out specific floating values to keep (flatten)\n",
    "    5. Return results\n",
    "\n",
    "3. With the above results, write them to disk if it was a full dataset (because it takes a while) or display them to verify that we're doing the right thing!\n",
    "\n",
    "4. Finally, cooalte the different results from each sentiment processor into a final data dictionary that other notebook scripts can utilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Started processing for model 'yelp'... === \n",
      "=== Started processing for model 'care'... === \n",
      "Started processing data... (5075 of 5075 samples)\n",
      "=== Started processing for model 'twitter'... === \n",
      "Started processing data... (5075 of 5075 samples)\n"
     ]
    }
   ],
   "source": [
    "# first, we define a helper function that will load a model and call it against data\n",
    "def call_sentiment_helper(model_name, df_eval, col_process, max_process_items, config, wrapped_model=None):\n",
    "    # load model from disk, see that it is a nicely \"wrapped\" model\n",
    "    model_remote_param = config[\"sentiment\"][model_name]\n",
    "    model_dump = config[\"publish\"][\"name_model3\"]+\"_\"+model_remote_param[\"style\"]\n",
    "    \n",
    "    # we allow the model to be passed because (a) they're all the same, (b) threading breaks with sessions\n",
    "    if wrapped_model is None:\n",
    "        wrapped_model = load_model(os.path.join('data', model_dump))\n",
    "\n",
    "    # although there are a few text columns, we'll just send the the column `reviewText` in for analysis\n",
    "    # NOTE: we're \"wrapping\" the one column as well for standard calling structure\n",
    "    #    nd_sample = [ [text1], [text2], ... ]\n",
    "    idx_access = list(range(len(df_eval)))\n",
    "    if max_process_items != 0:   # 0 special case for EVERYTHING\n",
    "        np.random.shuffle(idx_access)\n",
    "        idx_access = idx_access[:min(len(idx_access), max_process_items)]\n",
    "    print(\"Started processing data... ({} of {} samples)\".format(len(idx_access), len(df_eval)))\n",
    "    nd_sample = [[wrap_item] for wrap_item in df_eval.iloc[idx_access][col_process].values.tolist()]\n",
    "    list_result = util_call.score_model(wrapped_model, nd_sample, False,\n",
    "                        name_function=model_remote_param[\"api\"],\n",
    "                        url_remote=\"{}:{}\".format(\n",
    "                            config[\"sentiment\"][\"deploy_host\"], model_remote_param[\"port\"]))\n",
    "    df_result = pd.DataFrame(list_result, index=df_eval.index[idx_access])\n",
    "    \n",
    "    # now pull out the iteresting parts according to known style/output\n",
    "    if model_remote_param[\"style\"] == \"text-to-float\":\n",
    "        df_result = pd.DataFrame(df_result[\"FloatOut\"], index=df_eval.index[idx_access])\n",
    "    # other styles....\n",
    "    return df_result\n",
    "\n",
    "def helper_thread(model_name, wrapped_model=None):\n",
    "    df_local = {}\n",
    "    print(\"=== Started processing for model '{}'... === \".format(model_name))\n",
    "    if not os.path.exists(config[\"sentiment\"][model_name][\"path\"]) or max_process_items!=0:\n",
    "        df_local[\"X_test\"] = call_sentiment_helper(model_name, df[\"X_test\"], \n",
    "            sentiment[\"col_sentiment\"], max_process_items, config, wrapped_model=wrapped_model)\n",
    "\n",
    "        df_local[\"X_train_raw\"] = call_sentiment_helper(model_name, df[\"X_train_raw\"], \n",
    "            sentiment[\"col_sentiment\"], max_process_items, config, wrapped_model=wrapped_model)\n",
    "        \n",
    "        # write out our larger datasets as binary files\n",
    "        if max_process_items==0:    # only write full datasets\n",
    "            with gzip.open(config[\"sentiment\"][model_name][\"path\"], 'wb') as f:\n",
    "                pickle.dump(df_local, f)\n",
    "        # show a preview of what was just done...\n",
    "        print(\"... sample for model '{}'\".format(model_name))\n",
    "        print(df_local[\"X_train_raw\"].join(df[\"X_train_raw\"][sentiment[\"col_sentiment\"]]).sample(3))\n",
    "\n",
    "# okay, let's get ready to call our helper function for requested models\n",
    "sentiment = {}\n",
    "sentiment[\"X_test\"] = pd.DataFrame([], index=df[\"X_test\"].index)\n",
    "sentiment[\"X_train_raw\"] = pd.DataFrame([], index=df[\"X_train_raw\"].index)\n",
    "sentiment[\"col_sentiment\"] = \"reviewText\"\n",
    "\n",
    "# truncate range for faster evaluation\n",
    "max_process_items = 0     # set to 0 for everything (warning it might take a while)\n",
    "\n",
    "# actual evaluation code...\n",
    "thread_list = []\n",
    "thread_utilize = True\n",
    "# load model (WARNING: if you needed another model style than what was created, you may need to rework this)\n",
    "wrapped_model = load_model(os.path.join('data', model_dump))\n",
    "# evaluate models that are activated/available\n",
    "for model_name in config[\"sentiment\"][\"active_model\"]:\n",
    "    if thread_utilize:       # creating thread\n",
    "        t1 = threading.Thread(target=helper_thread, args=(model_name,wrapped_model)) \n",
    "        t1.start()\n",
    "        thread_list.append(t1)\n",
    "    else:\n",
    "        helper_thread(model_name,wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 250...\n",
      "Sample 250...\n",
      "Sample 500...\n",
      "Output error (http://acumos-gpu.research.att.com:8763), exception (500 Server Error: INTERNAL SERVER ERROR for url: http://acumos-gpu.research.att.com:8763/classify)\n",
      "Sample 500...\n",
      "Sample 750...\n",
      "Sample 750...\n",
      "Sample 1000...\n",
      "Sample 1000...\n"
     ]
    }
   ],
   "source": [
    "# wait for all threads to terminate\n",
    "for i in range(len(thread_list)):\n",
    "    thread_list[i].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combining and writing combined features to output ETL file...\")\n",
    "# now read the processed samples into our main dataframe\n",
    "for model_name in config[\"sentiment\"][\"active_model\"]:\n",
    "    if os.path.exists(config[\"sentiment\"][model_name][\"path\"]):\n",
    "        with gzip.open(config[\"sentiment\"][model_name][\"path\"], 'rb') as f:\n",
    "            df_local = pickle.load(f)\n",
    "            for k in df_local:\n",
    "                sentiment[k][model_name] = df_local[k]\n",
    "with gzip.open(config[\"path\"][\"sentiment\"], 'wb') as f:\n",
    "    pickle.dump(sentiment, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
