{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Processing ETL\n",
    "Example notebook for reading and transforming review data.  We read a source, remove SPI, and randomly split into known portions for training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # data read\n",
    "from sklearn import preprocessing  # data ETL\n",
    "from sklearn.model_selection import train_test_split   # balanced partioning\n",
    "import os,sys  # file checks\n",
    "import pickle   # compressed results\n",
    "import gzip  # compression \n",
    "import yaml   # configuration file\n",
    "from sklearn.feature_extraction import text  # text processing\n",
    "from datetime import datetime  # time processing\n",
    "import ast # help with JSON parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Options\n",
    "\n",
    "It's handy to include configuration options in a standard file that can be quickly modified and rerun if you're training something new.  Of course, you can always use command-line configurations as well, but a handy set of defaults in a human-readable file might be a bit easier when you're running things in notebooks.\n",
    "\n",
    "Here, we're using a simple [YAML](https://camel.readthedocs.io/en/latest/yamlref.html) file for our options which is human-readable, allows comments, and is well supported by other languages.\n",
    "\n",
    "To modify this program's operation, just open the file `config.yaml` in your editor of choice and rerun this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config.yaml'\n",
    "if not os.path.isfile(config_path):\n",
    "    print(\"Sorry, can't find the configuration file {}, aborting.\".format(config_path))\n",
    "    sys.exit(-1)\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "First, let's load our data to see if we need to perform any transform operations.  We will use built-in [JSON reading functions from pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html) that will parse json files into rows and columns to return a standardized [pandas dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). \n",
    "\n",
    "Of course, you could use whatever library or load function you're used to, but these dataframes have nice interoperability properties with other libraries for learning and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18784</th>\n",
       "      <td>B008FYGKJI</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>3</td>\n",
       "      <td>This is a basic multi-function printer.The set...</td>\n",
       "      <td>01 10, 2013</td>\n",
       "      <td>AHS6PX6H22WW1</td>\n",
       "      <td>H. Wang \"jwangamazon\"</td>\n",
       "      <td>A basic multi-function printer with two-side p...</td>\n",
       "      <td>1357776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13617</th>\n",
       "      <td>B004FQRW9W</td>\n",
       "      <td>[4, 6]</td>\n",
       "      <td>4</td>\n",
       "      <td>I can verify what some other reviewers have no...</td>\n",
       "      <td>01 24, 2012</td>\n",
       "      <td>A25UZ7MA72SMKM</td>\n",
       "      <td>Brent Butler</td>\n",
       "      <td>Powerful but may not stand the test of time</td>\n",
       "      <td>1327363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20854</th>\n",
       "      <td>B00BEYXGNY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>We use lots of colored paper for our art const...</td>\n",
       "      <td>07 2, 2013</td>\n",
       "      <td>A1TS45JWJVOSSW</td>\n",
       "      <td>Duane Sparks \"Duane\"</td>\n",
       "      <td>Not the best for art projects</td>\n",
       "      <td>1372723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22735</th>\n",
       "      <td>B00E5JUEF8</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>What else do you need?  The pen performs as a ...</td>\n",
       "      <td>12 30, 2013</td>\n",
       "      <td>A18LBGL7L9FEZV</td>\n",
       "      <td>Larry</td>\n",
       "      <td>Pen and stylus in one</td>\n",
       "      <td>1388361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11939</th>\n",
       "      <td>B003KGBGO0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>The entire back side of this mini white board ...</td>\n",
       "      <td>04 3, 2014</td>\n",
       "      <td>A13BX9O5UDBILC</td>\n",
       "      <td>Jong Lee</td>\n",
       "      <td>strong magnet backing</td>\n",
       "      <td>1396483200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin helpful  overall  \\\n",
       "18784  B008FYGKJI  [2, 3]        3   \n",
       "13617  B004FQRW9W  [4, 6]        4   \n",
       "20854  B00BEYXGNY  [0, 0]        3   \n",
       "22735  B00E5JUEF8  [1, 1]        5   \n",
       "11939  B003KGBGO0  [0, 0]        5   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "18784  This is a basic multi-function printer.The set...  01 10, 2013   \n",
       "13617  I can verify what some other reviewers have no...  01 24, 2012   \n",
       "20854  We use lots of colored paper for our art const...   07 2, 2013   \n",
       "22735  What else do you need?  The pen performs as a ...  12 30, 2013   \n",
       "11939  The entire back side of this mini white board ...   04 3, 2014   \n",
       "\n",
       "           reviewerID           reviewerName  \\\n",
       "18784   AHS6PX6H22WW1  H. Wang \"jwangamazon\"   \n",
       "13617  A25UZ7MA72SMKM           Brent Butler   \n",
       "20854  A1TS45JWJVOSSW   Duane Sparks \"Duane\"   \n",
       "22735  A18LBGL7L9FEZV                  Larry   \n",
       "11939  A13BX9O5UDBILC               Jong Lee   \n",
       "\n",
       "                                                 summary  unixReviewTime  \n",
       "18784  A basic multi-function printer with two-side p...      1357776000  \n",
       "13617        Powerful but may not stand the test of time      1327363200  \n",
       "20854                      Not the best for art projects      1372723200  \n",
       "22735                              Pen and stylus in one      1388361600  \n",
       "11939                              strong magnet backing      1396483200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.isfile(config[\"path\"][\"raw\"]):\n",
    "    print(\"Sorry, can't find the raw input file {}, aborting.\".format(config[\"path\"][\"raw\"]))\n",
    "    os.exit(-1)\n",
    "\n",
    "df_raw = pd.read_json(config[\"path\"][\"raw\"], orient=\"records\", lines=True)\n",
    "df_raw.sample(5) # handy/pretty preview function within notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Join\n",
    "This is an okay set of data, but we can actually pull in some other metadata to add more product information.  As with real databases and data feeds, this may occasionally be necessary, but there should always be a \"key\" or  column that uniquely identifies rows.  Here, it's the column `asin` which is the inventory or product number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "      <th>imUrl</th>\n",
       "      <th>price</th>\n",
       "      <th>related</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72949</th>\n",
       "      <td>B004PXMSSK</td>\n",
       "      <td></td>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Pa...</td>\n",
       "      <td>The word expressed in these cards can be used ...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/31iGdoPj...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{'Health &amp; Personal Care': 454062}</td>\n",
       "      <td>Atsui Cards, Box Set of 6 Note Cards, Sympathy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48836</th>\n",
       "      <td>B002T45X20</td>\n",
       "      <td></td>\n",
       "      <td>[Office Products, Office &amp; School Supplies, De...</td>\n",
       "      <td>Cooler Master Storm CS-M Weapon of Choice M4 D...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41izeWLp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Cooler Master Storm CS-M Weapon of Choice M4 D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106545</th>\n",
       "      <td>B009P15RUS</td>\n",
       "      <td>Scotch&amp;reg;</td>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Ta...</td>\n",
       "      <td>Scotch Magic Greener Tape 812-24P contains 24 ...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/414a9JiP...</td>\n",
       "      <td>25.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Scotch Magic Greener Tape, 3/4 x 900 Inches (8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6970</th>\n",
       "      <td>B000095S4P</td>\n",
       "      <td>Neenah</td>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Pa...</td>\n",
       "      <td></td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41xGcdpV...</td>\n",
       "      <td>12.93</td>\n",
       "      <td>{'also_bought': ['B000J0B91U', 'B006X3PWV0', '...</td>\n",
       "      <td></td>\n",
       "      <td>Neenah Astrobrights Premium Color Card Stock, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96555</th>\n",
       "      <td>B0083TQJFU</td>\n",
       "      <td></td>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Ca...</td>\n",
       "      <td></td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61kTmdVC...</td>\n",
       "      <td>11.64</td>\n",
       "      <td>{'also_bought': ['1449415911']}</td>\n",
       "      <td></td>\n",
       "      <td>Thomas Kinkade Gardens of Grace with Scripture...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              asin        brand  \\\n",
       "72949   B004PXMSSK                \n",
       "48836   B002T45X20                \n",
       "106545  B009P15RUS  Scotch&reg;   \n",
       "6970    B000095S4P       Neenah   \n",
       "96555   B0083TQJFU                \n",
       "\n",
       "                                               categories  \\\n",
       "72949   [Office Products, Office & School Supplies, Pa...   \n",
       "48836   [Office Products, Office & School Supplies, De...   \n",
       "106545  [Office Products, Office & School Supplies, Ta...   \n",
       "6970    [Office Products, Office & School Supplies, Pa...   \n",
       "96555   [Office Products, Office & School Supplies, Ca...   \n",
       "\n",
       "                                              description  \\\n",
       "72949   The word expressed in these cards can be used ...   \n",
       "48836   Cooler Master Storm CS-M Weapon of Choice M4 D...   \n",
       "106545  Scotch Magic Greener Tape 812-24P contains 24 ...   \n",
       "6970                                                        \n",
       "96555                                                       \n",
       "\n",
       "                                                    imUrl  price  \\\n",
       "72949   http://ecx.images-amazon.com/images/I/31iGdoPj...          \n",
       "48836   http://ecx.images-amazon.com/images/I/41izeWLp...          \n",
       "106545  http://ecx.images-amazon.com/images/I/414a9JiP...   25.0   \n",
       "6970    http://ecx.images-amazon.com/images/I/41xGcdpV...  12.93   \n",
       "96555   http://ecx.images-amazon.com/images/I/61kTmdVC...  11.64   \n",
       "\n",
       "                                                  related  \\\n",
       "72949                                                       \n",
       "48836                                                       \n",
       "106545                                                      \n",
       "6970    {'also_bought': ['B000J0B91U', 'B006X3PWV0', '...   \n",
       "96555                     {'also_bought': ['1449415911']}   \n",
       "\n",
       "                                 salesRank  \\\n",
       "72949   {'Health & Personal Care': 454062}   \n",
       "48836                                        \n",
       "106545                                       \n",
       "6970                                         \n",
       "96555                                        \n",
       "\n",
       "                                                    title  \n",
       "72949   Atsui Cards, Box Set of 6 Note Cards, Sympathy...  \n",
       "48836   Cooler Master Storm CS-M Weapon of Choice M4 D...  \n",
       "106545  Scotch Magic Greener Tape, 3/4 x 900 Inches (8...  \n",
       "6970    Neenah Astrobrights Premium Color Card Stock, ...  \n",
       "96555   Thomas Kinkade Gardens of Grace with Scripture...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.isfile(config[\"path\"][\"metadata\"]):\n",
    "    print(\"Sorry, can't find the metadata input file {}, aborting.\".format(config[\"path\"][\"metadata\"]))\n",
    "    os.exit(-1)\n",
    "\n",
    "# a bit of a quirk, the metadata here uses single quotes (ugh!), which is not standard json\n",
    "# so we must first load and transform that data; see the tip at https://stackoverflow.com/a/48593076\n",
    "with gzip.open(config[\"path\"][\"metadata\"], \"rt\") as f:\n",
    "    df_meta = pd.DataFrame(ast.literal_eval(\"[\"+f.read().replace('\\n', ',').replace('\\r', '')+\"]\")).fillna('').astype(str)\n",
    "    # also go from a string version of array to actual string array\n",
    "    df_meta[\"categories\"] = df_meta[\"categories\"].apply(lambda x: ast.literal_eval(x.lower())[0])\n",
    "    \n",
    "df_meta.sample(5) # handy/pretty preview function within notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw samples (25374), metadata records (134838)\n",
      "Combined samples (25374)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "      <th>imUrl</th>\n",
       "      <th>price</th>\n",
       "      <th>related</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B004UMNN2G</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>This is a good pack of hanging file folders. I...</td>\n",
       "      <td>09 22, 2011</td>\n",
       "      <td>A2K89R0B20LYHB</td>\n",
       "      <td>Christine</td>\n",
       "      <td>Color useful but paper is thin</td>\n",
       "      <td>1316649600</td>\n",
       "      <td>Pendaflex</td>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Fi...</td>\n",
       "      <td>New and Improved Ready Tab. Superior Durabilit...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41OHA5RM...</td>\n",
       "      <td>11.33</td>\n",
       "      <td>{'also_bought': ['B00016UVP2', 'B000WXBDZG', '...</td>\n",
       "      <td></td>\n",
       "      <td>Pendaflex Ready-Tab Hanging File Folder, Assor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B0039MZZGK</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I'll be about the 500th reviewer to say \"cute\"...</td>\n",
       "      <td>04 19, 2011</td>\n",
       "      <td>A37D2TGTIXRV2N</td>\n",
       "      <td>plyopowerd \"Arrow Dynamic Mom\"</td>\n",
       "      <td>Sweet accessory for shoe or Dorothy fans</td>\n",
       "      <td>1303171200</td>\n",
       "      <td>Scotch</td>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Ta...</td>\n",
       "      <td>In 1930, 3M engineer Richard Drew invented an ...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41F3JOcZ...</td>\n",
       "      <td>6.99</td>\n",
       "      <td>{'also_bought': ['B003VNE25M', 'B0086ZL1E0', '...</td>\n",
       "      <td></td>\n",
       "      <td>Red Shoe Scotch Magic Tape Dispenser, 3/4 x 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00AHV7MJO</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Oh, I loathe those little plastic tabs that go...</td>\n",
       "      <td>04 28, 2013</td>\n",
       "      <td>A1UQBFCERIP7VJ</td>\n",
       "      <td>Margaret Picky</td>\n",
       "      <td>Brilliant!</td>\n",
       "      <td>1367107200</td>\n",
       "      <td>Smead</td>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Fi...</td>\n",
       "      <td>Erasable FasTab hanging folders have a special...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41vkqOGv...</td>\n",
       "      <td>15.69</td>\n",
       "      <td>{'also_bought': ['B0013COEEW', 'B000GRA91W', '...</td>\n",
       "      <td></td>\n",
       "      <td>Smead Erasable FasTab Hanging Folders, 1/3-Cut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B007XPBW4I</th>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>The last time I moved, the movers charged me t...</td>\n",
       "      <td>07 2, 2013</td>\n",
       "      <td>A5GPH59NDWJRB</td>\n",
       "      <td>Jenna of the Jungle</td>\n",
       "      <td>Sturdy</td>\n",
       "      <td>1372723200</td>\n",
       "      <td>Bankers Box</td>\n",
       "      <td>[Office Products, Office &amp; School Supplies, En...</td>\n",
       "      <td>Wardrobe boxes are designed specifically for m...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41w1E4jc...</td>\n",
       "      <td>65.7</td>\n",
       "      <td>{'also_bought': ['B002A9JQSG', 'B007XPBW3Y', '...</td>\n",
       "      <td></td>\n",
       "      <td>Bankers Box SmoothMove Wardrobe Box, 24 x 24 x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B0006OF5MI</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>I had expected this sorter to be of the accord...</td>\n",
       "      <td>04 21, 2014</td>\n",
       "      <td>ADY836HK6QSYR</td>\n",
       "      <td>ardnam \"ardnam\"</td>\n",
       "      <td>Not exactly what I thought</td>\n",
       "      <td>1398038400</td>\n",
       "      <td>Wilson Jones</td>\n",
       "      <td>[Office Products, Office &amp; School Supplies, Fi...</td>\n",
       "      <td>Book style expandable sorter easily organizes ...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/411mWImP...</td>\n",
       "      <td>14.33</td>\n",
       "      <td>{'also_bought': ['B002Q8HXZ4', 'B000J09O1W', '...</td>\n",
       "      <td></td>\n",
       "      <td>Wilson Jones Favorite Desk File/Sorter, A-Z In...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           helpful  overall  \\\n",
       "asin                          \n",
       "B004UMNN2G  [0, 0]        4   \n",
       "B0039MZZGK  [0, 0]        5   \n",
       "B00AHV7MJO  [0, 0]        5   \n",
       "B007XPBW4I  [1, 1]        5   \n",
       "B0006OF5MI  [0, 0]        3   \n",
       "\n",
       "                                                   reviewText   reviewTime  \\\n",
       "asin                                                                         \n",
       "B004UMNN2G  This is a good pack of hanging file folders. I...  09 22, 2011   \n",
       "B0039MZZGK  I'll be about the 500th reviewer to say \"cute\"...  04 19, 2011   \n",
       "B00AHV7MJO  Oh, I loathe those little plastic tabs that go...  04 28, 2013   \n",
       "B007XPBW4I  The last time I moved, the movers charged me t...   07 2, 2013   \n",
       "B0006OF5MI  I had expected this sorter to be of the accord...  04 21, 2014   \n",
       "\n",
       "                reviewerID                    reviewerName  \\\n",
       "asin                                                         \n",
       "B004UMNN2G  A2K89R0B20LYHB                       Christine   \n",
       "B0039MZZGK  A37D2TGTIXRV2N  plyopowerd \"Arrow Dynamic Mom\"   \n",
       "B00AHV7MJO  A1UQBFCERIP7VJ                  Margaret Picky   \n",
       "B007XPBW4I   A5GPH59NDWJRB             Jenna of the Jungle   \n",
       "B0006OF5MI   ADY836HK6QSYR                 ardnam \"ardnam\"   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "asin                                                                   \n",
       "B004UMNN2G            Color useful but paper is thin      1316649600   \n",
       "B0039MZZGK  Sweet accessory for shoe or Dorothy fans      1303171200   \n",
       "B00AHV7MJO                                Brilliant!      1367107200   \n",
       "B007XPBW4I                                    Sturdy      1372723200   \n",
       "B0006OF5MI                Not exactly what I thought      1398038400   \n",
       "\n",
       "                   brand                                         categories  \\\n",
       "asin                                                                          \n",
       "B004UMNN2G     Pendaflex  [Office Products, Office & School Supplies, Fi...   \n",
       "B0039MZZGK        Scotch  [Office Products, Office & School Supplies, Ta...   \n",
       "B00AHV7MJO         Smead  [Office Products, Office & School Supplies, Fi...   \n",
       "B007XPBW4I   Bankers Box  [Office Products, Office & School Supplies, En...   \n",
       "B0006OF5MI  Wilson Jones  [Office Products, Office & School Supplies, Fi...   \n",
       "\n",
       "                                                  description  \\\n",
       "asin                                                            \n",
       "B004UMNN2G  New and Improved Ready Tab. Superior Durabilit...   \n",
       "B0039MZZGK  In 1930, 3M engineer Richard Drew invented an ...   \n",
       "B00AHV7MJO  Erasable FasTab hanging folders have a special...   \n",
       "B007XPBW4I  Wardrobe boxes are designed specifically for m...   \n",
       "B0006OF5MI  Book style expandable sorter easily organizes ...   \n",
       "\n",
       "                                                        imUrl  price  \\\n",
       "asin                                                                   \n",
       "B004UMNN2G  http://ecx.images-amazon.com/images/I/41OHA5RM...  11.33   \n",
       "B0039MZZGK  http://ecx.images-amazon.com/images/I/41F3JOcZ...   6.99   \n",
       "B00AHV7MJO  http://ecx.images-amazon.com/images/I/41vkqOGv...  15.69   \n",
       "B007XPBW4I  http://ecx.images-amazon.com/images/I/41w1E4jc...   65.7   \n",
       "B0006OF5MI  http://ecx.images-amazon.com/images/I/411mWImP...  14.33   \n",
       "\n",
       "                                                      related salesRank  \\\n",
       "asin                                                                      \n",
       "B004UMNN2G  {'also_bought': ['B00016UVP2', 'B000WXBDZG', '...             \n",
       "B0039MZZGK  {'also_bought': ['B003VNE25M', 'B0086ZL1E0', '...             \n",
       "B00AHV7MJO  {'also_bought': ['B0013COEEW', 'B000GRA91W', '...             \n",
       "B007XPBW4I  {'also_bought': ['B002A9JQSG', 'B007XPBW3Y', '...             \n",
       "B0006OF5MI  {'also_bought': ['B002Q8HXZ4', 'B000J09O1W', '...             \n",
       "\n",
       "                                                        title  \n",
       "asin                                                           \n",
       "B004UMNN2G  Pendaflex Ready-Tab Hanging File Folder, Assor...  \n",
       "B0039MZZGK  Red Shoe Scotch Magic Tape Dispenser, 3/4 x 35...  \n",
       "B00AHV7MJO  Smead Erasable FasTab Hanging Folders, 1/3-Cut...  \n",
       "B007XPBW4I  Bankers Box SmoothMove Wardrobe Box, 24 x 24 x...  \n",
       "B0006OF5MI  Wilson Jones Favorite Desk File/Sorter, A-Z In...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we join by ASIN to the raw data\n",
    "print(\"Raw samples ({}), metadata records ({})\".format(len(df_raw), len(df_meta)))\n",
    "df_raw = df_raw.set_index(\"asin\").join(df_meta.set_index(\"asin\"), on=\"asin\", how=\"left\")\n",
    "df_meta = None\n",
    "print(\"Combined samples ({})\".format(len(df_raw)))\n",
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpful                                                      [0, 0]\n",
      "overall                                                           5\n",
      "reviewText        This is a really good, high-quality product.  ...\n",
      "reviewTime                                              10 29, 2010\n",
      "reviewerID                                           A1P2XYD265YE21\n",
      "reviewerName                                    Andrea \"Readaholic\"\n",
      "summary                                                 Really Good\n",
      "unixReviewTime                                           1288310400\n",
      "brand                                                         Avery\n",
      "categories        [Office Products, Office & School Supplies, Pa...\n",
      "description       Perfect for invitations, thank you notes, movi...\n",
      "imUrl             http://ecx.images-amazon.com/images/I/51q11PWY...\n",
      "price                                                          8.42\n",
      "related           {'also_bought': ['B0000AQNVK', 'B00007E7CW', '...\n",
      "salesRank                                                          \n",
      "title             Avery Half-Fold Greeting Cards for Inkjet Prin...\n",
      "Name: B00000JFNV, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_raw.iloc[0]) # take a closer look at just one row/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "To avoid data bias and contamination, let's break apart the data into `training` and `testing` (aka `evaluation`) sets. These partitions help us to avoid the problems of over-fitting and under-fitting while training for our problem.  Additionally, by doing the segmentation this early in model building, we'll be able to keep consistent samples for comparing different models that are evaluated.  In the special function `train_test_split` we specify the column `overall` so that we get a set of balanced classes in our training and testing data.\n",
    "\n",
    "In some works, a third split often called `validation` can be used for parameter tuning after training but before testing on unseen data.\n",
    "\n",
    "Here are a few quick ETL steps we're performing...\n",
    "\n",
    "1. We also see that a few fields, `reviewerID` and `reviewerName` are likely SPI (sensitive personal information) that we don't need in any of our analysis, so we will summarily `delete` it. \n",
    "\n",
    "2. There is a redundant field `reviewTime` that is a repeated text form of `unixReviewTime` so we can delete it, too.\n",
    "\n",
    "3. We don't want our model to learn anything specific to unique products, so we will also drop the column `brand`, `price`, `salesRank`, `imUrl`, `related`, `title`, and `asin`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpful                                                      [1, 1]\n",
      "overall                                                           4\n",
      "reviewText        Oh my Avery, how I LOVE you!!  I am a total ne...\n",
      "summary           Another wonderful Avery product that feeds my ...\n",
      "unixReviewTime                                           1287619200\n",
      "categories        [Office Products, Office & School Supplies, La...\n",
      "description                                                        \n",
      "Name: 665, dtype: object\n",
      "Dimensionality before processing (20299, 7)\n"
     ]
    }
   ],
   "source": [
    "if \"reviewerID\" in df_raw.columns:\n",
    "    df_raw.drop([\"reviewerID\", \"reviewerName\", \"reviewTime\", \"brand\", \"price\", \"salesRank\", \"imUrl\", \"related\", \"title\"], inplace=True, axis=1)\n",
    "df_raw.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# now compare to the prior data above\n",
    "df = {}\n",
    "df[\"X_train\"], df[\"X_test\"] = train_test_split(df_raw, stratify=df_raw[\"overall\"],\n",
    "                                        test_size=config[\"partition\"][\"test\"], \n",
    "                                        random_state=0)\n",
    "models = {}  # this will allow us to persist models\n",
    "df[\"X_test_enc\"] = df[\"X_test\"].copy()\n",
    "print(df[\"X_train\"].iloc[0]) # take a closer look at just one row/sample (after dropping)\n",
    "print(\"Dimensionality before processing {}\".format(df[\"X_train\"].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "From the above explorations, we can see that we'll need to transform the data into a numerical representation before we can continue. \n",
    "\n",
    "\n",
    "### Textual Transforms\n",
    "For text data, there are a million ways you can go from text to numerical featues, with ealy, tried and true techniques like [frequency vectors](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) and [hashing vectors](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) where all words are counted and then assigned to a fixed feature dimension to more complex methods like [word embedding](https://en.wikipedia.org/wiki/Word_embedding).  \n",
    "\n",
    "There are also tons of pre-processing steps that can be added like [stemming](https://pythonspot.com/nltk-stemming/) and [stop word removal](https://pythonspot.com/nltk-stop-words/), but those additional steps are left as exercises for the reader.\n",
    "\n",
    "In this sample ETL code, we'll extract [normalized TFIDF counts](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) with English stop words and a maximum number of textual features, which is just one step above the simpler counts alone.  A richer demonstration with this library can be [found here](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py).\n",
    "\n",
    "### Categorical Transforms\n",
    "For categorical data, there are two popular methods to encode non-numerical data: [one hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) and simply [label encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder).  \n",
    "\n",
    "Label encoding is the simpler of the two and it enumerates all of your known values into numerical values.  It offers smaller final dimensionality and if the values are releated, it might make sense to keep them ordinally related.  For example, mapping the quality grades \"poor, acceptable, good\" into a single ordinal.\n",
    "```\n",
    "    transform([['apple']; ['orange']; ['bannana']]) --> [[0]; [1]; [2]]\n",
    "```\n",
    "\n",
    "One-hot encoding also enumerates all known values into binary vectors.  Some classifiers learn depenencies between numerical values, so assigning several unrelated numbers could actually hurt your case.\n",
    "```\n",
    "    transform([['apple']; ['orange']; ['bannana']]) --> [[1 0 0]; [0 1 0]; [0 0 1]]\n",
    "```\n",
    "\n",
    "This data actually has the column `categories` which can contain multiple categories (e.g. `[\"chair\",\"office\"]`, so we'll run the transform there but use a [MultiLabelBinarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) which works like one-hot encoding, but allows multiple items to be simultaneously active.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text column 'reviewText'...\n",
      "... dimensionality after processing (20299, 506)\n",
      "Preprocessing text column 'summary'...\n",
      "... dimensionality after processing (20299, 1005)\n",
      "Preprocessing text column 'description'...\n",
      "... dimensionality after processing (20299, 1504)\n",
      "Preprocessing vectorized 'helpful' ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train text models for both big text columns\n",
    "for c in [\"reviewText\",\"summary\",\"description\"]:\n",
    "    print(\"Preprocessing text column '{}'...\".format(c))\n",
    "    encN = \"enc_text_{}\".format(c)\n",
    "    encC = \"enc_cols_{}\".format(c)\n",
    "    # create and train vectorizer\n",
    "    models[encN] = text.TfidfVectorizer(\n",
    "        stop_words=\"english\", max_features=config[\"encoding\"][\"max_text_terms\"])\n",
    "    models[encN].fit(df[\"X_train\"][c])\n",
    "    # encode new features to dataframe\n",
    "    models[encC] = [\"{}_{}\".format(c[0:2], n) for n in models[encN].get_feature_names()]\n",
    "    tmp_encode = pd.DataFrame(models[encN].transform(df[\"X_train\"][c]).toarray(), \n",
    "                              index=df[\"X_train\"].index, columns=models[encC])    \n",
    "    df[\"X_train\"] = pd.concat([df[\"X_train\"], tmp_encode], axis=1, sort=False)\n",
    "    # repeat for test data (except we didn't train with it)\n",
    "    tmp_encode = pd.DataFrame(models[encN].transform(df[\"X_test_enc\"][c]).toarray(), \n",
    "                              index=df[\"X_test_enc\"].index, columns=models[encC])    \n",
    "    df[\"X_test_enc\"] = pd.concat([df[\"X_test_enc\"], tmp_encode], axis=1, sort=False)\n",
    "    # delete prior columns\n",
    "    df[\"X_train\"].drop(c, axis=1, inplace=True)\n",
    "    df[\"X_test_enc\"].drop(c, axis=1, inplace=True)\n",
    "    print(\"... dimensionality after processing {}\".format(df[\"X_train\"].shape))\n",
    "\n",
    "# now, normalize the list of helpfulness of the review scores \n",
    "#   (e.g. [1,2] which is \"Yes/No\") into a l1 unit vector of counts [0.2, 0.8] \n",
    "print(\"Preprocessing vectorized 'helpful' ...\")\n",
    "tmp_encode = preprocessing.normalize(list(df[\"X_train\"][\"helpful\"]), norm='l1')\n",
    "tmp_encode = pd.DataFrame(tmp_encode, columns=[\"help_0\", \"help_1\"], index=df[\"X_train\"].index)\n",
    "df[\"X_train\"] = pd.concat([df[\"X_train\"], tmp_encode], axis=1, sort=False)\n",
    "tmp_encode = preprocessing.normalize(list(df[\"X_test_enc\"][\"helpful\"]), norm='l1')\n",
    "tmp_encode = pd.DataFrame(tmp_encode, columns=[\"help_0\", \"help_1\"], index=df[\"X_test_enc\"].index)\n",
    "df[\"X_test_enc\"] = pd.concat([df[\"X_test_enc\"], tmp_encode], axis=1, sort=False)\n",
    "# delete prior columns\n",
    "df[\"X_train\"].drop(\"helpful\", axis=1, inplace=True)\n",
    "df[\"X_test_enc\"].drop(\"helpful\", axis=1, inplace=True)\n",
    "\n",
    "# one more tweak is to pull out 'day of week' from our timestamp\n",
    "#   generally, you'd want to disregard all time information for a fair learning\n",
    "#   but some studies demonstrate social trends over time\n",
    "# https://hbr.org/2018/10/research-why-ratings-on-everything-from-wine-to-amazon-products-improve-over-time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing one-hot category columns...\n",
      "... dimensionality after processing (20299, 1713)\n"
     ]
    }
   ],
   "source": [
    "# specify the columns of interest\n",
    "print(\"Preprocessing one-hot category columns...\")\n",
    "models[\"col_hot\"] = \"categories\"\n",
    "# train and transform columns\n",
    "models[\"enc_hot\"] = preprocessing.MultiLabelBinarizer()\n",
    "models[\"enc_hot\"].fit(df[\"X_train\"][models[\"col_hot\"]])\n",
    "# compute new column names after encoding\n",
    "models[\"col_hot_enc\"] = [\"cat_{}\".format(x) for x in models[\"enc_hot\"].classes_]\n",
    "# combine into new larger matrix\n",
    "x = models[\"enc_hot\"].transform(df[\"X_train\"][models[\"col_hot\"]])\n",
    "tmp_encode = pd.DataFrame(models[\"enc_hot\"].transform(df[\"X_train\"][models[\"col_hot\"]]), \n",
    "                          index=df[\"X_train\"].index, columns=models[\"col_hot_enc\"])\n",
    "df[\"X_train\"] = pd.concat([df[\"X_train\"], tmp_encode], axis=1, sort=False)\n",
    "tmp_encode = pd.DataFrame(models[\"enc_hot\"].transform(df[\"X_test_enc\"][models[\"col_hot\"]]), \n",
    "                          index=df[\"X_test_enc\"].index, columns=models[\"col_hot_enc\"])\n",
    "df[\"X_test_enc\"] = pd.concat([df[\"X_test_enc\"], tmp_encode], axis=1, sort=False)\n",
    "# delete prior columns for raw data\n",
    "df[\"X_train\"].drop(models[\"col_hot\"], axis=1, inplace=True)\n",
    "df[\"X_test_enc\"].drop(models[\"col_hot\"], axis=1, inplace=True)\n",
    "print(\"... dimensionality after processing {}\".format(df[\"X_train\"].shape))\n",
    "\n",
    "# another way to do category encoding with just two values (e.g. On/Off)\n",
    "# # specify the columns of interest\n",
    "# models[\"col_binary\"] = [\"Partner\", \"Dependents\", \"PhoneService\", \"InternetService\", \\\n",
    "#                   \"PaperlessBilling\", \"Churn\"]\n",
    "# # train and transform columns\n",
    "# models[\"enc_binary\"] = preprocessing.OrdinalEncoder()  # this is a multi-feature version of LabelEncoder\n",
    "# models[\"enc_binary\"].fit(df[\"X_train\"][models[\"col_binary\"]])\n",
    "# # overwrite previous data with transformation\n",
    "# df[\"X_train\"][models[\"col_binary\"]] = models[\"enc_binary\"].transform(df[\"X_train\"][models[\"col_binary\"]])\n",
    "# df[\"X_test_enc\"][models[\"col_binary\"]] = models[\"enc_binary\"].transform(X_tdf[\"X_test_enc\"]est_enc[models[\"col_binary\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Transforms\n",
    "One more tweak is to pull out 'day of week' from our timestamp. Generally, you'd want to disregard all time information for a fair learning but some [studies demonstrate positive skews over time](https://hbr.org/2018/10/research-why-ratings-on-everything-from-wine-to-amazon-products-improve-over-time).  \n",
    "\n",
    "Here, we're keeping both the raw timestamp (to account for above) and a day of week as an example of how to process and extract a time segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing vectorized 'unixReviewTime' ...\n",
      "... dimensionality after processing (20299, 1714)\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing vectorized 'unixReviewTime' ...\")\n",
    "df_time = pd.to_datetime(df[\"X_train\"][\"unixReviewTime\"], unit='s')\n",
    "df[\"X_train\"][\"reviewDOW\"] = df_time.dt.dayofweek\n",
    "df_time = pd.to_datetime(df[\"X_test_enc\"][\"unixReviewTime\"], unit='s')\n",
    "df[\"X_test_enc\"][\"reviewDOW\"] = df_time.dt.dayofweek\n",
    "print(\"... dimensionality after processing {}\".format(df[\"X_train\"].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excise your labels\n",
    "Don't forget to exclude class labels from the training data or else you'll get 100% accuracy and probably a slap on the wrist from your model development friends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dimensionality after processing (20299, 1713)\n"
     ]
    }
   ],
   "source": [
    "# finally, let's grab our label column (\"Churn\") that we're trying to predict\n",
    "df[\"y_train\"] = df[\"X_train\"][\"overall\"]\n",
    "del df[\"X_train\"][\"overall\"]\n",
    "df[\"y_test\"] = df[\"X_test_enc\"][\"overall\"]\n",
    "del df[\"X_test\"][\"overall\"]\n",
    "del df[\"X_test_enc\"][\"overall\"]\n",
    "\n",
    "print(\"Final dimensionality after processing {}\".format(df[\"X_train\"].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>re_10</th>\n",
       "      <th>re_100</th>\n",
       "      <th>re_11</th>\n",
       "      <th>re_12</th>\n",
       "      <th>re_20</th>\n",
       "      <th>re_30</th>\n",
       "      <th>re_34</th>\n",
       "      <th>re_3m</th>\n",
       "      <th>re_8217</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_Utility Carts</th>\n",
       "      <th>cat_Velcro &amp; Mounting Products</th>\n",
       "      <th>cat_View Binders</th>\n",
       "      <th>cat_VoIP</th>\n",
       "      <th>cat_Wall Calendars</th>\n",
       "      <th>cat_Wirebound Notebooks</th>\n",
       "      <th>cat_Wooden Colored Pencils</th>\n",
       "      <th>cat_Wrist Rests</th>\n",
       "      <th>cat_Writing &amp; Correction Supplies</th>\n",
       "      <th>reviewDOW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>1.287619e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>0.047167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23034</th>\n",
       "      <td>1.393200e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8253</th>\n",
       "      <td>1.264032e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.558033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>1.304554e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22147</th>\n",
       "      <td>1.379203e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14291</th>\n",
       "      <td>1.313539e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11506</th>\n",
       "      <td>1.292285e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696</th>\n",
       "      <td>1.292285e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7324</th>\n",
       "      <td>1.287619e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16380</th>\n",
       "      <td>1.331770e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  1713 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unixReviewTime  re_10  re_100  re_11     re_12     re_20  re_30  re_34  \\\n",
       "665      1.287619e+09    0.0     0.0    0.0  0.048788  0.047167    0.0    0.0   \n",
       "23034    1.393200e+09    0.0     0.0    0.0  0.000000  0.000000    0.0    0.0   \n",
       "8253     1.264032e+09    0.0     0.0    0.0  0.000000  0.000000    0.0    0.0   \n",
       "1963     1.304554e+09    0.0     0.0    0.0  0.000000  0.000000    0.0    0.0   \n",
       "22147    1.379203e+09    0.0     0.0    0.0  0.000000  0.000000    0.0    0.0   \n",
       "14291    1.313539e+09    0.0     0.0    0.0  0.000000  0.000000    0.0    0.0   \n",
       "11506    1.292285e+09    0.0     0.0    0.0  0.000000  0.000000    0.0    0.0   \n",
       "3696     1.292285e+09    0.0     0.0    0.0  0.000000  0.000000    0.0    0.0   \n",
       "7324     1.287619e+09    0.0     0.0    0.0  0.000000  0.000000    0.0    0.0   \n",
       "16380    1.331770e+09    0.0     0.0    0.0  0.000000  0.000000    0.0    0.0   \n",
       "\n",
       "          re_3m  re_8217    ...      cat_Utility Carts  \\\n",
       "665    0.000000      0.0    ...                    0.0   \n",
       "23034  0.000000      0.0    ...                    0.0   \n",
       "8253   0.558033      0.0    ...                    0.0   \n",
       "1963   0.000000      0.0    ...                    0.0   \n",
       "22147  0.000000      0.0    ...                    0.0   \n",
       "14291  0.000000      0.0    ...                    0.0   \n",
       "11506  0.000000      0.0    ...                    0.0   \n",
       "3696   0.000000      0.0    ...                    0.0   \n",
       "7324   0.000000      0.0    ...                    0.0   \n",
       "16380  0.000000      0.0    ...                    0.0   \n",
       "\n",
       "       cat_Velcro & Mounting Products  cat_View Binders  cat_VoIP  \\\n",
       "665                               0.0               0.0       0.0   \n",
       "23034                             0.0               0.0       0.0   \n",
       "8253                              0.0               0.0       0.0   \n",
       "1963                              0.0               0.0       0.0   \n",
       "22147                             0.0               0.0       0.0   \n",
       "14291                             0.0               0.0       0.0   \n",
       "11506                             0.0               0.0       0.0   \n",
       "3696                              0.0               0.0       0.0   \n",
       "7324                              0.0               0.0       0.0   \n",
       "16380                             0.0               0.0       0.0   \n",
       "\n",
       "       cat_Wall Calendars  cat_Wirebound Notebooks  \\\n",
       "665                   0.0                      0.0   \n",
       "23034                 0.0                      0.0   \n",
       "8253                  0.0                      0.0   \n",
       "1963                  0.0                      0.0   \n",
       "22147                 0.0                      0.0   \n",
       "14291                 0.0                      0.0   \n",
       "11506                 0.0                      0.0   \n",
       "3696                  0.0                      0.0   \n",
       "7324                  0.0                      0.0   \n",
       "16380                 0.0                      0.0   \n",
       "\n",
       "       cat_Wooden Colored Pencils  cat_Wrist Rests  \\\n",
       "665                           0.0              0.0   \n",
       "23034                         0.0              0.0   \n",
       "8253                          0.0              0.0   \n",
       "1963                          0.0              0.0   \n",
       "22147                         0.0              0.0   \n",
       "14291                         0.0              0.0   \n",
       "11506                         0.0              0.0   \n",
       "3696                          0.0              0.0   \n",
       "7324                          0.0              0.0   \n",
       "16380                         0.0              0.0   \n",
       "\n",
       "       cat_Writing & Correction Supplies  reviewDOW  \n",
       "665                                  0.0        3.0  \n",
       "23034                                0.0        0.0  \n",
       "8253                                 0.0        3.0  \n",
       "1963                                 0.0        3.0  \n",
       "22147                                0.0        6.0  \n",
       "14291                                0.0        2.0  \n",
       "11506                                0.0        1.0  \n",
       "3696                                 0.0        1.0  \n",
       "7324                                 0.0        3.0  \n",
       "16380                                0.0        3.0  \n",
       "\n",
       "[10 rows x 1713 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally we should able to cast all data to a double format\n",
    "# however, some values for \"TotalCharges\" were missing, so set them to zero\n",
    "df[\"X_train\"] = df[\"X_train\"].apply(pd.to_numeric, errors='corce').fillna(0).astype(float)\n",
    "df[\"X_test_enc\"] = df[\"X_test_enc\"].apply(pd.to_numeric, errors='corce').fillna(0).astype(float)\n",
    "\n",
    "# now compare to the prior data above\n",
    "df[\"X_train\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created encoded training set 20299x1713 (labels 20299) and raw test set 5075x6 (partition size 0.2) \n"
     ]
    }
   ],
   "source": [
    "# finally, let's save all of this data to disk\n",
    "df[\"X_train\"].sample(100).to_csv(config[\"path\"][\"example\"], index=False)\n",
    "\n",
    "# write out our larger datasets as binary files\n",
    "with gzip.open(config[\"path\"][\"etl\"], 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "# and write out our intermediate model data (in case we need to transform again)\n",
    "with open(config[\"path\"][\"model\"], 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "\n",
    "# write out some stats\n",
    "print(\"Created encoded training set {}x{} (labels {}) \" \\\n",
    "      \"and raw test set {}x{} (partition size {}) \".format( \\\n",
    "    len(df[\"X_train\"]), len(df[\"X_train\"].columns), len(df[\"y_train\"]), \n",
    "    len(df[\"X_test\"]), len(df[\"X_test\"].columns), config[\"partition\"][\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and scaling\n",
    "There are other handy tricks to [normalize or scale](https://en.wikipedia.org/wiki/Normalization_(statistics)) your data, but we'll leave that as an exercise for the future.  Some learning algorithms (like deep neural networks) will derive features on their own so it isn't as necessary to preprocess the raw values in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
